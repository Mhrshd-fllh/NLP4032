{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## Importing Required Libraries"
      ],
      "metadata": {
        "id": "Qej_7d3w8YLs"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_RiGPH178Vly"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import gensim\n",
        "import gensim.downloader as api\n",
        "from gensim.models import Word2Vec\n",
        "from gensim.models.word2vec import Text8Corpus\n",
        "from IPython.display import display, Markdown\n",
        "from sklearn.decomposition import PCA\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Load the Text8 Dataset\n",
        "The **Text8 dataset** is a large-scale dataset extracted from Wikipedia, often used for training word embeddings.  \n",
        "It consists of approximately 17 million words in a single text file, formatted as a sequence of words without punctuation.\n",
        "\n",
        "In this step, you will:\n",
        "1. **Download the Text8 dataset** using Gensim's downloader.\n",
        "2. **Convert it to a list named text8_corpus** to use it for training."
      ],
      "metadata": {
        "id": "w_SEJi7Z-q6S"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Your Code Here"
      ],
      "metadata": {
        "id": "tDfqKSux-MaW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(text8_corpus)"
      ],
      "metadata": {
        "id": "zIESaD1D_Svh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Train a Word2Vec Model in CBOW Mode\n",
        "\n",
        "In this step, you will initialize a Word2Vec model and train it using a specific architecture.  \n",
        "The model will learn word representations based on a **context-predicting approach**, where surrounding words help predict the target word.  \n",
        "\n",
        "Consider the impact of different parameters such as:\n",
        "- The size of the word representations(e.g., 300 dimensions).\n",
        "- The number of neighboring words considered(e.g., 5 neighbors).\n",
        "- The minimum occurrences required for a word to be included in the vocabulary(e.g., 5 occurrences).\n",
        "- The number of CPU cores used for training(e.g., 4 cores).\n",
        "\n",
        "Run the code below to train the model for **one epoch**.\n"
      ],
      "metadata": {
        "id": "pt176_i3_WyJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Your Code Here"
      ],
      "metadata": {
        "id": "S7Z8I_EW_WRx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Example Words\n",
        "We select a few words of interest."
      ],
      "metadata": {
        "id": "uQcJaSCoAxB3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "words = [\"king\", \"queen\", \"man\", \"woman\", \"car\", \"bus\"]"
      ],
      "metadata": {
        "id": "MF7B3TZkApV3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Compare Word Similarities\n",
        "\n",
        "After training the Word2Vec model, we can analyze how well it captures relationships between words.  \n",
        "In this step, we:\n",
        "- Find the **top 5 most similar words** for each based on cosine similarity.\n",
        "- Display the results in a structured format.\n",
        "\n",
        "Words with high similarity scores are expected to have similar meanings or occur in similar contexts.  \n",
        "If a word is **not found in the vocabulary**, it means it didnâ€™t meet the minimum occurrence threshold during training.\n"
      ],
      "metadata": {
        "id": "rNGQ0iWqBLnk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for word in words:\n",
        "    if ## code\n",
        "        similar_words = ## code\n",
        "        display(Markdown(f\"**{word}:**\"))\n",
        "        for w, score in similar_words:\n",
        "            display(Markdown(f\"- {w}: {score:.4f}\"))\n",
        "    else:\n",
        "        display(Markdown(f\"{word} not in vocabulary.\"))"
      ],
      "metadata": {
        "id": "MJY8WIl1BIEr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Visualizing Word Embeddings\n",
        "\n",
        "Word embeddings are high-dimensional vectors, making them difficult to interpret directly.  \n",
        "To visualize them, we use **Principal Component Analysis (PCA)** to reduce their dimensionality from 300 to 2.\n",
        "\n",
        "### Steps:\n",
        "1. Extract word vectors for selected words.\n",
        "2. Apply **PCA** to reduce dimensionality from 300 to 2.\n",
        "3. Plot the words in a 2D space.\n",
        "\n",
        "Words that appear in similar contexts should be **closer together** in the plot.\n"
      ],
      "metadata": {
        "id": "7UKf4_dFDQmh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "vectors = ## CODE\n",
        "pca = ## CODE\n",
        "result = ## CODE\n",
        "plt.figure(figsize=(8,6))\n",
        "plt.scatter(## CODE)\n",
        "for i, word in enumerate(words):\n",
        "    ## CODE\n",
        "plt.title(\"Fine-Tuned Word Embeddings Visualization using CBOW\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "5JxfpH5jA3-4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Training Word2Vec with Skip-gram\n",
        "\n",
        "Now, we will train another Word2Vec model using a different architecture called **Skip-gram**.  \n",
        "Unlike the previous CBOW approach, Skip-gram learns to predict **context words** given a target word, making it more effective for learning representations of rare words.\n",
        "\n",
        "Repeat the previous steps:\n",
        "1. Initialize a new Word2Vec model with a different training mode.\n",
        "2. Train it using the **Text8 dataset** for one epoch.\n"
      ],
      "metadata": {
        "id": "4r2wugcyEWwM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Your Code Here"
      ],
      "metadata": {
        "id": "gc_1RwDtEZp2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Compare Word Similarities (Skip-gram)\n",
        "Now, we repeat the similarity comparison, but this time using the **Skip-gram** model.\n",
        "\n",
        "- Skip-gram focuses on predicting surrounding words given a target word.\n",
        "- It is better suited for smaller datasets and learns high-quality embeddings, especially for infrequent words.\n",
        "- Here, we retrieve and display the most similar words for a given set of words.\n"
      ],
      "metadata": {
        "id": "0lSQp9c_EcLE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for word in words:\n",
        "    if ## code\n",
        "        similar_words = ## code\n",
        "        display(Markdown(f\"**{word}:**\"))\n",
        "        for w, score in similar_words:\n",
        "            display(Markdown(f\"- {w}: {score:.4f}\"))\n",
        "    else:\n",
        "        display(Markdown(f\"{word} not in vocabulary.\"))"
      ],
      "metadata": {
        "id": "NyAwow1-FG16"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Visualizing Word Embeddings (Skip-gram)\n",
        "Now, we visualize the word embeddings obtained from the **Skip-gram** model."
      ],
      "metadata": {
        "id": "1_2AyIIeFR0W"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "vectors = ## CODE\n",
        "pca = ## CODE\n",
        "result = ## CODE\n",
        "plt.figure(figsize=(8,6))\n",
        "plt.scatter(## CODE)\n",
        "for i, word in enumerate(words):\n",
        "    ## CODE\n",
        "plt.title(\"Fine-Tuned Word Embeddings Visualization using CBOW\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "kz27vqbsFj8-"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}