{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UL3KIKrzvjcd"
      },
      "source": [
        "### Word2Vec Implementation from Scratch\n",
        "\n",
        "# Introduction\n",
        " Word2Vec is a popular technique for word embeddings, which captures the meaning of words by placing them in a continuous vector space.\n",
        " In this exercise, you will implement Word2Vec using NumPy and complete the missing parts of the code.\n",
        "We will represent each word as a one-hot vector, meaning each word in the vocabulary is mapped to a unique binary vector with only one active (1) position."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5gpCfRgivsvG"
      },
      "source": [
        "import necessary libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 67,
      "metadata": {
        "id": "O5uKnkUJvJDq"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from collections import defaultdict"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 123,
      "metadata": {
        "id": "Dzazztv_wtbI"
      },
      "outputs": [],
      "source": [
        "### Adjust the hyperparameters if needed ###\n",
        "settings = {\n",
        "\t'window_size': 2,\n",
        "\t'n': 4,\n",
        "\t'epochs': 150,\n",
        "\t'learning_rate': 0.0001\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 124,
      "metadata": {
        "id": "GlpwAgpnv6tY"
      },
      "outputs": [],
      "source": [
        "class word2vec:\n",
        "    def __init__(self, settings):\n",
        "        \"\"\"\n",
        "        Initialize the Word2Vec model with given hyperparameters.\n",
        "        \"\"\"\n",
        "        ## Start code\n",
        "        self.n = settings['n']\n",
        "        self.lr = settings['learning_rate']\n",
        "        self.epochs = settings['epochs']\n",
        "        self.window = settings['window_size']\n",
        "        ## End code\n",
        "\n",
        "    def generate_training_data(self, corpus):\n",
        "        \"\"\"\n",
        "        Generate training data from the given corpus.\n",
        "        This function processes the input corpus to create training examples for the Word2Vec model.\n",
        "        - It first counts the occurrences of each word in the corpus.\n",
        "        - Then, it creates a vocabulary of unique words and assigns each word a unique index.\n",
        "        - Finally, it generates training pairs consisting of target words and their surrounding context words.\n",
        "        \"\"\"\n",
        "        ## Start code\n",
        "        # Each word frequency in the corpus\n",
        "        word_counts = defaultdict(int)\n",
        "        for sentence in corpus:\n",
        "          for word in sentence:\n",
        "            word_counts[word] += 1\n",
        "\n",
        "\n",
        "        self.v_count = len(word_counts.keys()) # Total Vocab len\n",
        "        self.words_list = list(word_counts.keys()) # Vocabs\n",
        "        self.word_index = {word: i for i, word in enumerate(self.words_list)} # Each vocab index\n",
        "        self.index_word = {i: word for word, i in self.word_index.items()} # #Each index vocab\n",
        "        training_data = []\n",
        "        # Generating Word Context pairs in each sentence of corpus.\n",
        "        # We process Each word in the sentecne and gets its index in word_index list and look at window before and after that vocab.\n",
        "        # At the end we add the pairs into our training_data\n",
        "        for sentence in corpus:\n",
        "          for i, word in enumerate(sentence):\n",
        "            id = self.word_index[word]\n",
        "            start = max(0, i - self.window)\n",
        "            end = min(len(sentence), i + self.window + 1)\n",
        "            for j in range(start, end):\n",
        "              if i != j:\n",
        "                curr_word = sentence[j]\n",
        "                training_data.append([id, self.word_index[curr_word]])\n",
        "\n",
        "        training_data = np.array(training_data)\n",
        "        return training_data\n",
        "        ## End code\n",
        "\n",
        "    def word2onehot(self, word):\n",
        "        \"\"\"\n",
        "        Convert a word into a one-hot encoded vector.\n",
        "        Output:\n",
        "        - A one-hot vector of length equal to the vocabulary size.\n",
        "        \"\"\"\n",
        "        ## Start code\n",
        "        # one hot for i : [0 0 0 0 .. 1 (i-th index) 0 .. 0]\n",
        "        word_vec = np.zeros(self.v_count)\n",
        "        word_vec[self.word_index[word]] = 1\n",
        "        return word_vec\n",
        "        ## End code\n",
        "\n",
        "    def train(self, training_data):\n",
        "        \"\"\"\n",
        "        Train the model using the given training data.\n",
        "        This function initializes the weight matrices and performs forward and backward propagation.\n",
        "        - Initializes weight matrices w1 (input to hidden) and w2 (hidden to output)\n",
        "        - Iterates through training data and performs forward pass\n",
        "        - Computes the error and updates weights using backpropagation\n",
        "        - Tracks and prints loss for each epoch\n",
        "        \"\"\"\n",
        "        ## Start code\n",
        "        # Two matrices for weights (initialized values will be random)\n",
        "        self.w1 = np.random.uniform(-1, 1, (self.v_count, self.n))\n",
        "        self.w2 = np.random.uniform(-1, 1, (self.n, self.v_count))\n",
        "\n",
        "        for epoch in range(self.epochs):\n",
        "          loss = 0\n",
        "          for target, context in training_data:\n",
        "            x = self.word2onehot(self.index_word[target])\n",
        "            y_true = self.word2onehot(self.index_word[context])\n",
        "            y_pred, h, u = self.forward_pass(x)\n",
        "            e = y_true - y_pred\n",
        "            self.backprop(e, h, x)\n",
        "\n",
        "            loss += -np.sum(y_true * np.log(y_pred + 1e-9))\n",
        "          print('Epoch:', epoch + 1, 'Loss:', loss.round(4))\n",
        "\n",
        "        ## End code\n",
        "\n",
        "    def softmax(self, x):\n",
        "        \"\"\"\n",
        "        Apply softmax function.\n",
        "        This function normalizes the input values into probabilities, ensuring that they sum to 1.\n",
        "        - It exponentiates each value in x to ensure non-negativity.\n",
        "        - It divides each exponentiated value by the sum of all exponentiated values to normalize them into a probability distribution.\n",
        "        Output:\n",
        "        - A probability distribution where the sum of all elements equals 1.\n",
        "        \"\"\"\n",
        "        ## Start code\n",
        "        e_x = np.exp(x - np.max(x))\n",
        "        e_x = e_x / np.sum(e_x)\n",
        "        return e_x\n",
        "        ## End code\n",
        "\n",
        "    def forward_pass(self, x):\n",
        "        \"\"\"\n",
        "        Forward pass through the network.\n",
        "        This function takes a one-hot encoded word vector as input and performs the following steps:\n",
        "        - Computes the hidden layer by multiplying the input vector with the first weight matrix.\n",
        "        - Computes the output layer values by multiplying the hidden layer with the second weight matrix.\n",
        "        - Applies the softmax function to get the probability distribution over the vocabulary.\n",
        "        Output:\n",
        "        - The predicted probability distribution (y_c), hidden layer activations (h), and raw scores before softmax (u).\n",
        "        \"\"\"\n",
        "        ## Start code\n",
        "        h = np.dot(x, self.w1)\n",
        "        u = np.dot(h, self.w2)\n",
        "        y_c = self.softmax(u)\n",
        "        return y_c, h, u\n",
        "        ## End code\n",
        "\n",
        "    def backprop(self, e, h, x):\n",
        "        \"\"\"\n",
        "        Backpropagation step to update weights.\n",
        "        This function updates the weight matrices using gradient descent.\n",
        "        - Computes the gradient of the loss with respect to the second weight matrix (w2).\n",
        "        - Computes the gradient of the loss with respect to the first weight matrix (w1).\n",
        "        - Updates w1 and w2 using the learning rate and computed gradients.\n",
        "        \"\"\"\n",
        "        ## Start code\n",
        "        dl_dw2 = np.outer(h, e)\n",
        "        dl_dw1 = np.outer(x, np.dot(self.w2, e))\n",
        "        self.w1 -= self.lr * dl_dw1\n",
        "        self.w2 -= self.lr * dl_dw2\n",
        "        ## End code\n",
        "\n",
        "    def word_vec(self, word):\n",
        "        \"\"\"\n",
        "        Retrieve the word vector for a given word.\n",
        "        \"\"\"\n",
        "        ## Start code\n",
        "        v_w = self.w1[self.word_index[word]]\n",
        "\n",
        "        return v_w\n",
        "        ## End code\n",
        "\n",
        "    def vec_sim(self, word, top_n):\n",
        "        \"\"\"\n",
        "        Find top N most similar words based on cosine similarity.\n",
        "        \"\"\"\n",
        "        ## Start code\n",
        "        v_w1 = self.word_vec(word)\n",
        "        word_sim = {}\n",
        "\n",
        "\n",
        "        for other_word in self.words_list:\n",
        "          if other_word != word:\n",
        "            v_w2 = self.word_vec(other_word)\n",
        "            similarity = np.dot(v_w1, v_w2) / (np.linalg.norm(v_w1) * np.linalg.norm(v_w2)) # Cosine Similairity\n",
        "            word_sim[other_word] = similarity\n",
        "\n",
        "        words_sorted = sorted(word_sim.items(), key=lambda x: x[1], reverse=True)\n",
        "        for word, sim in words_sorted[:top_n]:\n",
        "            print(word, sim)\n",
        "        ## End code"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 125,
      "metadata": {
        "id": "OrARMd5i6Zqt"
      },
      "outputs": [],
      "source": [
        "text = \"Natural language processing and machine learning open up fascinating possibilities, allowing machines to analyze,\\\n",
        " understand, and respond to human language in ways that were once thought impossible.\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 129,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q7ntj1Es6u2x",
        "outputId": "f712af56-7afd-4123-8fdb-c4d40643b413"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 1 Loss: 336.4113\n",
            "Epoch: 2 Loss: 336.4363\n",
            "Epoch: 3 Loss: 336.4614\n",
            "Epoch: 4 Loss: 336.4864\n",
            "Epoch: 5 Loss: 336.5115\n",
            "Epoch: 6 Loss: 336.5367\n",
            "Epoch: 7 Loss: 336.5618\n",
            "Epoch: 8 Loss: 336.587\n",
            "Epoch: 9 Loss: 336.6121\n",
            "Epoch: 10 Loss: 336.6373\n",
            "Epoch: 11 Loss: 336.6626\n",
            "Epoch: 12 Loss: 336.6878\n",
            "Epoch: 13 Loss: 336.713\n",
            "Epoch: 14 Loss: 336.7383\n",
            "Epoch: 15 Loss: 336.7636\n",
            "Epoch: 16 Loss: 336.7889\n",
            "Epoch: 17 Loss: 336.8142\n",
            "Epoch: 18 Loss: 336.8396\n",
            "Epoch: 19 Loss: 336.865\n",
            "Epoch: 20 Loss: 336.8904\n",
            "Epoch: 21 Loss: 336.9158\n",
            "Epoch: 22 Loss: 336.9412\n",
            "Epoch: 23 Loss: 336.9666\n",
            "Epoch: 24 Loss: 336.9921\n",
            "Epoch: 25 Loss: 337.0176\n",
            "Epoch: 26 Loss: 337.0431\n",
            "Epoch: 27 Loss: 337.0686\n",
            "Epoch: 28 Loss: 337.0942\n",
            "Epoch: 29 Loss: 337.1197\n",
            "Epoch: 30 Loss: 337.1453\n",
            "Epoch: 31 Loss: 337.1709\n",
            "Epoch: 32 Loss: 337.1965\n",
            "Epoch: 33 Loss: 337.2222\n",
            "Epoch: 34 Loss: 337.2478\n",
            "Epoch: 35 Loss: 337.2735\n",
            "Epoch: 36 Loss: 337.2992\n",
            "Epoch: 37 Loss: 337.3249\n",
            "Epoch: 38 Loss: 337.3507\n",
            "Epoch: 39 Loss: 337.3764\n",
            "Epoch: 40 Loss: 337.4022\n",
            "Epoch: 41 Loss: 337.428\n",
            "Epoch: 42 Loss: 337.4538\n",
            "Epoch: 43 Loss: 337.4797\n",
            "Epoch: 44 Loss: 337.5055\n",
            "Epoch: 45 Loss: 337.5314\n",
            "Epoch: 46 Loss: 337.5573\n",
            "Epoch: 47 Loss: 337.5832\n",
            "Epoch: 48 Loss: 337.6092\n",
            "Epoch: 49 Loss: 337.6351\n",
            "Epoch: 50 Loss: 337.6611\n",
            "Epoch: 51 Loss: 337.6871\n",
            "Epoch: 52 Loss: 337.7131\n",
            "Epoch: 53 Loss: 337.7392\n",
            "Epoch: 54 Loss: 337.7652\n",
            "Epoch: 55 Loss: 337.7913\n",
            "Epoch: 56 Loss: 337.8174\n",
            "Epoch: 57 Loss: 337.8435\n",
            "Epoch: 58 Loss: 337.8697\n",
            "Epoch: 59 Loss: 337.8958\n",
            "Epoch: 60 Loss: 337.922\n",
            "Epoch: 61 Loss: 337.9482\n",
            "Epoch: 62 Loss: 337.9744\n",
            "Epoch: 63 Loss: 338.0007\n",
            "Epoch: 64 Loss: 338.0269\n",
            "Epoch: 65 Loss: 338.0532\n",
            "Epoch: 66 Loss: 338.0795\n",
            "Epoch: 67 Loss: 338.1059\n",
            "Epoch: 68 Loss: 338.1322\n",
            "Epoch: 69 Loss: 338.1586\n",
            "Epoch: 70 Loss: 338.185\n",
            "Epoch: 71 Loss: 338.2114\n",
            "Epoch: 72 Loss: 338.2378\n",
            "Epoch: 73 Loss: 338.2643\n",
            "Epoch: 74 Loss: 338.2907\n",
            "Epoch: 75 Loss: 338.3172\n",
            "Epoch: 76 Loss: 338.3437\n",
            "Epoch: 77 Loss: 338.3703\n",
            "Epoch: 78 Loss: 338.3968\n",
            "Epoch: 79 Loss: 338.4234\n",
            "Epoch: 80 Loss: 338.45\n",
            "Epoch: 81 Loss: 338.4766\n",
            "Epoch: 82 Loss: 338.5032\n",
            "Epoch: 83 Loss: 338.5299\n",
            "Epoch: 84 Loss: 338.5566\n",
            "Epoch: 85 Loss: 338.5833\n",
            "Epoch: 86 Loss: 338.61\n",
            "Epoch: 87 Loss: 338.6368\n",
            "Epoch: 88 Loss: 338.6635\n",
            "Epoch: 89 Loss: 338.6903\n",
            "Epoch: 90 Loss: 338.7171\n",
            "Epoch: 91 Loss: 338.7439\n",
            "Epoch: 92 Loss: 338.7708\n",
            "Epoch: 93 Loss: 338.7977\n",
            "Epoch: 94 Loss: 338.8246\n",
            "Epoch: 95 Loss: 338.8515\n",
            "Epoch: 96 Loss: 338.8784\n",
            "Epoch: 97 Loss: 338.9054\n",
            "Epoch: 98 Loss: 338.9324\n",
            "Epoch: 99 Loss: 338.9594\n",
            "Epoch: 100 Loss: 338.9864\n",
            "Epoch: 101 Loss: 339.0134\n",
            "Epoch: 102 Loss: 339.0405\n",
            "Epoch: 103 Loss: 339.0676\n",
            "Epoch: 104 Loss: 339.0947\n",
            "Epoch: 105 Loss: 339.1218\n",
            "Epoch: 106 Loss: 339.149\n",
            "Epoch: 107 Loss: 339.1761\n",
            "Epoch: 108 Loss: 339.2033\n",
            "Epoch: 109 Loss: 339.2306\n",
            "Epoch: 110 Loss: 339.2578\n",
            "Epoch: 111 Loss: 339.2851\n",
            "Epoch: 112 Loss: 339.3123\n",
            "Epoch: 113 Loss: 339.3397\n",
            "Epoch: 114 Loss: 339.367\n",
            "Epoch: 115 Loss: 339.3943\n",
            "Epoch: 116 Loss: 339.4217\n",
            "Epoch: 117 Loss: 339.4491\n",
            "Epoch: 118 Loss: 339.4765\n",
            "Epoch: 119 Loss: 339.504\n",
            "Epoch: 120 Loss: 339.5314\n",
            "Epoch: 121 Loss: 339.5589\n",
            "Epoch: 122 Loss: 339.5864\n",
            "Epoch: 123 Loss: 339.6139\n",
            "Epoch: 124 Loss: 339.6415\n",
            "Epoch: 125 Loss: 339.6691\n",
            "Epoch: 126 Loss: 339.6967\n",
            "Epoch: 127 Loss: 339.7243\n",
            "Epoch: 128 Loss: 339.7519\n",
            "Epoch: 129 Loss: 339.7796\n",
            "Epoch: 130 Loss: 339.8073\n",
            "Epoch: 131 Loss: 339.835\n",
            "Epoch: 132 Loss: 339.8627\n",
            "Epoch: 133 Loss: 339.8905\n",
            "Epoch: 134 Loss: 339.9182\n",
            "Epoch: 135 Loss: 339.946\n",
            "Epoch: 136 Loss: 339.9739\n",
            "Epoch: 137 Loss: 340.0017\n",
            "Epoch: 138 Loss: 340.0296\n",
            "Epoch: 139 Loss: 340.0575\n",
            "Epoch: 140 Loss: 340.0854\n",
            "Epoch: 141 Loss: 340.1133\n",
            "Epoch: 142 Loss: 340.1413\n",
            "Epoch: 143 Loss: 340.1693\n",
            "Epoch: 144 Loss: 340.1973\n",
            "Epoch: 145 Loss: 340.2253\n",
            "Epoch: 146 Loss: 340.2533\n",
            "Epoch: 147 Loss: 340.2814\n",
            "Epoch: 148 Loss: 340.3095\n",
            "Epoch: 149 Loss: 340.3376\n",
            "Epoch: 150 Loss: 340.3658\n"
          ]
        }
      ],
      "source": [
        "corpus = [[word.lower() for word in text.split()]]\n",
        "\n",
        "w2v = word2vec(settings)\n",
        "\n",
        "training_data = w2v.generate_training_data(corpus)\n",
        "\n",
        "w2v.train(training_data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 130,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fPbTmdZu6uJA",
        "outputId": "b51e0c26-1ee9-4381-a6e6-0ce074ccdc81"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "machine [ 0.9836496   1.00943166 -0.70331904  0.47491823]\n",
            "machines 0.7059625750623806\n",
            "fascinating 0.5771269101843289\n",
            "thought 0.5369287864606412\n"
          ]
        }
      ],
      "source": [
        "word = \"machine\"\n",
        "vec = w2v.word_vec(word)\n",
        "print(word, vec)\n",
        "\n",
        "# Find similar words\n",
        "w2v.vec_sim(\"machine\", 3)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t2tYYb3JZ7o8"
      },
      "source": [
        "# Code Logic\n",
        "## Word2Vec (By Skipgram)\n",
        "I used Skipgram to implement a code for getting word2vec and similarity by cosine similarity and also used a simple neural network for training this neural network. In skipgram model learns that if there is a central word which words will be around it based on the corpus trained on it.\n",
        "\n",
        "# Results\n",
        "\n",
        "<table>\n",
        "<tr>\n",
        "<td>n</td>\n",
        "<td>window</td>\n",
        "<td>learning rate</td>\n",
        "<td>epochs</td>\n",
        "<td>Last Loss</td>\n",
        "<td>3 Sims</td>\n",
        "</tr>\n",
        "<tr>\n",
        "<td>5</td>\n",
        "<td>2</td>\n",
        "<td>0.01</td>\n",
        "<td>50</td>\n",
        "<td>2072.3266</td>\n",
        "<td>fascinating - to - language</td>\n",
        "</tr>\n",
        "<tr>\n",
        "<td>10</td>\n",
        "<td>4</td>\n",
        "<td>0.001</td>\n",
        "<td>50</td>\n",
        "<td>1422.4607</td>\n",
        "<td>learning - up - that</td>\n",
        "</tr>\n",
        "<tr>\n",
        "<td>10</td>\n",
        "<td>6</td>\n",
        "<td>0.0005</td>\n",
        "<td>100</td>\n",
        "<td>1931.6857</td>\n",
        "<td>allowing - learning - open</td>\n",
        "</tr>\n",
        "<tr>\n",
        "<td>8</td>\n",
        "<td>5</td>\n",
        "<td>0.0005</td>\n",
        "<td>100</td>\n",
        "<td>880.6461</td>\n",
        "<td>analyze - open - that</td>\n",
        "</tr>\n",
        "<tr>\n",
        "<td>4</td>\n",
        "<td>2</td>\n",
        "<td>0.0001</td>\n",
        "<td>150</td>\n",
        "<td>340.3658</td>\n",
        "<td>machines - fascinating - thoughts</td>\n",
        "</tr>\n",
        "</table>\n",
        "\n",
        "Based on my experiments the best hyperparameters are <i><b>n = 4, window_size = 2, learning_rate = 0.0001 and epochs = 150</i></b>\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fQzO_LBegYoO"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
